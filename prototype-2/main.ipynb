{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eec997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JANPALLY SUSHEEL\\Desktop\\new del pxlb\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "import pywt\n",
    "import wfdb\n",
    "import ast\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b8c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for PTB-XL ECG data\"\"\"\n",
    "    \n",
    "    def __init__(self, signals, labels, transform=None):\n",
    "        self.signals = signals\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        signal = self.signals[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            signal = self.transform(signal)\n",
    "        \n",
    "        return torch.FloatTensor(signal), torch.FloatTensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e5ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"1D ResNet Block for ECG signals\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1, stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55f36393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet1D(nn.Module):\n",
    "    \"\"\"1D ResNet for ECG feature extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels=12, num_classes=5):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, 7, 2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.maxpool = nn.MaxPool1d(3, 2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, 256)\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ResNetBlock(in_channels, out_channels, stride=stride))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResNetBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a32fe389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWTFeatureExtractor:\n",
    "    \"\"\"Discrete Wavelet Transform feature extractor\"\"\"\n",
    "    \n",
    "    def __init__(self, wavelet='db4', levels=4):\n",
    "        self.wavelet = wavelet\n",
    "        self.levels = levels\n",
    "    \n",
    "    def extract_features(self, signal):\n",
    "        \"\"\"Extract DWT features from ECG signal\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for lead in range(signal.shape[0]):  # For each lead\n",
    "            lead_signal = signal[lead, :]\n",
    "            \n",
    "            # Perform DWT decomposition\n",
    "            coeffs = pywt.wavedec(lead_signal, self.wavelet, level=self.levels)\n",
    "            \n",
    "            # Extract statistical features from each level\n",
    "            for coeff in coeffs:\n",
    "                features.extend([\n",
    "                    np.mean(coeff),\n",
    "                    np.std(coeff),\n",
    "                    np.var(coeff),\n",
    "                    np.max(coeff),\n",
    "                    np.min(coeff),\n",
    "                    np.median(coeff)\n",
    "                ])\n",
    "        \n",
    "        return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c408fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGClassificationModel(nn.Module):\n",
    "    \"\"\"Complete ECG Classification Model combining ResNet and DWT features\"\"\"\n",
    "    \n",
    "    def __init__(self, resnet_features=256, dwt_features=360, num_classes=5):\n",
    "        super(ECGClassificationModel, self).__init__()\n",
    "        \n",
    "        # ResNet branch\n",
    "        self.resnet = ResNet1D()\n",
    "        \n",
    "        # Feature combination\n",
    "        self.feature_combiner = nn.Sequential(\n",
    "            nn.Linear(resnet_features + dwt_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Deep Neural Network\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes),\n",
    "            nn.Sigmoid()  # For multi-label classification\n",
    "        )\n",
    "    \n",
    "    def forward(self, ecg_signal, dwt_features):\n",
    "        # ResNet features\n",
    "        resnet_features = self.resnet(ecg_signal)\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = torch.cat([resnet_features, dwt_features], dim=1)\n",
    "        combined_features = self.feature_combiner(combined_features)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(combined_features)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05b20139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBXLDataProcessor:\n",
    "    \"\"\"PTB-XL dataset processor\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, sampling_rate=500):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.dwt_extractor = DWTFeatureExtractor()\n",
    "        \n",
    "        # Class mapping\n",
    "        self.class_mapping = {\n",
    "            'CD': 0,   # Conduction Disturbance\n",
    "            'HYP': 1,  # Hypertrophy\n",
    "            'MI': 2,   # Myocardial Infarction  \n",
    "            'NORM': 3, # Normal\n",
    "            'STTC': 4  # ST/T Change\n",
    "        }\n",
    "    \n",
    "    def load_database(self):\n",
    "        \"\"\"Load PTB-XL database CSV\"\"\"\n",
    "        # Try different possible file names and locations\n",
    "        possible_paths = [\n",
    "            self.data_path / 'ptbxl_database.csv',\n",
    "            self.data_path / 'ptb-xl_database.csv',\n",
    "            self.data_path / 'database.csv'\n",
    "        ]\n",
    "        \n",
    "        db_path = None\n",
    "        for path in possible_paths:\n",
    "            if path.exists():\n",
    "                db_path = path\n",
    "                break\n",
    "        \n",
    "        if db_path is None:\n",
    "            # List all CSV files in the directory\n",
    "            csv_files = list(self.data_path.glob('*.csv'))\n",
    "            print(f\"Available CSV files in {self.data_path}:\")\n",
    "            for file in csv_files:\n",
    "                print(f\"  - {file.name}\")\n",
    "            \n",
    "            # Try to find database file by looking for 'database' in name\n",
    "            database_files = [f for f in csv_files if 'database' in f.name.lower()]\n",
    "            if database_files:\n",
    "                db_path = database_files[0]\n",
    "                print(f\"Using database file: {db_path.name}\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Could not find database CSV file in {self.data_path}\")\n",
    "        \n",
    "        print(f\"Loading database from: {db_path}\")\n",
    "        self.df = pd.read_csv(db_path, index_col='ecg_id')\n",
    "        \n",
    "        # Convert scp_codes from string to dict\n",
    "        self.df.scp_codes = self.df.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "        \n",
    "        print(f\"Loaded {len(self.df)} ECG records\")\n",
    "        return self.df\n",
    "    \n",
    "    def load_scp_statements(self):\n",
    "        \"\"\"Load SCP statements for label mapping\"\"\"\n",
    "        # Try different possible file names\n",
    "        possible_paths = [\n",
    "            self.data_path / 'scp_statements.csv',\n",
    "            self.data_path / 'scp-statements.csv',\n",
    "            self.data_path / 'statements.csv'\n",
    "        ]\n",
    "        \n",
    "        scp_path = None\n",
    "        for path in possible_paths:\n",
    "            if path.exists():\n",
    "                scp_path = path\n",
    "                break\n",
    "        \n",
    "        if scp_path is None:\n",
    "            # List all CSV files that might be SCP statements\n",
    "            csv_files = list(self.data_path.glob('*.csv'))\n",
    "            scp_files = [f for f in csv_files if 'scp' in f.name.lower() or 'statement' in f.name.lower()]\n",
    "            if scp_files:\n",
    "                scp_path = scp_files[0]\n",
    "                print(f\"Using SCP statements file: {scp_path.name}\")\n",
    "            else:\n",
    "                print(\"Warning: Could not find scp_statements.csv file\")\n",
    "                print(\"Available CSV files:\")\n",
    "                for file in csv_files:\n",
    "                    print(f\"  - {file.name}\")\n",
    "                return None\n",
    "        \n",
    "        print(f\"Loading SCP statements from: {scp_path}\")\n",
    "        self.scp_df = pd.read_csv(scp_path, index_col=0)\n",
    "        return self.scp_df\n",
    "    \n",
    "    def extract_labels(self):\n",
    "        \"\"\"Extract labels from scp_codes\"\"\"\n",
    "        # Get diagnostic superclass\n",
    "        def get_diagnostic_class(scp_codes):\n",
    "            labels = np.zeros(5)  # CD, HYP, MI, NORM, STTC\n",
    "            \n",
    "            for code, _ in scp_codes.items():\n",
    "                if code in self.scp_df.index:\n",
    "                    diagnostic_class = self.scp_df.loc[code, 'diagnostic_class']\n",
    "                    if diagnostic_class in self.class_mapping:\n",
    "                        labels[self.class_mapping[diagnostic_class]] = 1\n",
    "            \n",
    "            return labels\n",
    "        \n",
    "        self.labels = np.array([get_diagnostic_class(codes) for codes in self.df.scp_codes])\n",
    "        print(f\"Label distribution:\\n{np.sum(self.labels, axis=0)}\")\n",
    "        return self.labels\n",
    "    \n",
    "    def load_signals(self, limit=None):\n",
    "        \"\"\"Load ECG signals\"\"\"\n",
    "        signals = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for idx, (ecg_id, row) in enumerate(tqdm(self.df.iterrows(), desc=\"Loading signals\")):\n",
    "            if limit and idx >= limit:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                # Load signal file\n",
    "                signal_path = self.data_path / row['filename_lr']\n",
    "                record = wfdb.rdsamp(str(signal_path.with_suffix('')))\n",
    "                signal = record[0].T  # Shape: (12, 5000)\n",
    "                \n",
    "                # Normalize per lead\n",
    "                signal = self.normalize_signal(signal)\n",
    "                signals.append(signal)\n",
    "                valid_indices.append(idx)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {ecg_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        self.signals = np.array(signals)\n",
    "        self.labels = self.labels[valid_indices]\n",
    "        \n",
    "        print(f\"Loaded {len(signals)} signals with shape {self.signals.shape}\")\n",
    "        return self.signals, self.labels\n",
    "    \n",
    "    def normalize_signal(self, signal):\n",
    "        \"\"\"Normalize ECG signal per lead\"\"\"\n",
    "        normalized = np.zeros_like(signal)\n",
    "        for lead in range(signal.shape[0]):\n",
    "            lead_signal = signal[lead, :]\n",
    "            normalized[lead, :] = (lead_signal - np.mean(lead_signal)) / (np.std(lead_signal) + 1e-8)\n",
    "        return normalized\n",
    "    \n",
    "    def extract_dwt_features(self):\n",
    "        \"\"\"Extract DWT features from all signals\"\"\"\n",
    "        print(\"Extracting DWT features...\")\n",
    "        dwt_features = []\n",
    "        \n",
    "        for signal in tqdm(self.signals):\n",
    "            features = self.dwt_extractor.extract_features(signal)\n",
    "            dwt_features.append(features)\n",
    "        \n",
    "        self.dwt_features = np.array(dwt_features)\n",
    "        print(f\"DWT features shape: {self.dwt_features.shape}\")\n",
    "        return self.dwt_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a79ea4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50, device='cuda'):\n",
    "    \"\"\"Train the ECG classification model\"\"\"\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for ecg_batch, dwt_batch, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            ecg_batch = ecg_batch.to(device)\n",
    "            dwt_batch = dwt_batch.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(ecg_batch, dwt_batch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for ecg_batch, dwt_batch, labels in val_loader:\n",
    "                ecg_batch = ecg_batch.to(device)\n",
    "                dwt_batch = dwt_batch.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(ecg_batch, dwt_batch)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0507aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device='cuda'):\n",
    "    \"\"\"Evaluate the model and return predictions\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ecg_batch, dwt_batch, labels in test_loader:\n",
    "            ecg_batch = ecg_batch.to(device)\n",
    "            dwt_batch = dwt_batch.to(device)\n",
    "            \n",
    "            outputs = model(ecg_batch, dwt_batch)\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5a227f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_with_shap(model, sample_data, device='cuda'):\n",
    "    \"\"\"Use SHAP for model interpretability\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create SHAP explainer\n",
    "    def model_wrapper(x):\n",
    "        ecg_signals = torch.FloatTensor(x[:, :, :5000]).to(device)  # ECG signals\n",
    "        dwt_features = torch.FloatTensor(x[:, 0, 5000:]).to(device)  # DWT features\n",
    "        return model(ecg_signals, dwt_features).cpu().detach().numpy()\n",
    "    \n",
    "    # Sample explanation (simplified for demonstration)\n",
    "    print(\"SHAP analysis would be performed here for model interpretability\")\n",
    "    print(\"This would show which ECG features contribute most to each prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c37ac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading database...\n",
      "Loading database from: C:\\Users\\JANPALLY SUSHEEL\\Desktop\\new del pxlb\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\\ptbxl_database.csv\n",
      "Loaded 21799 ECG records\n",
      "Loading SCP statements from: C:\\Users\\JANPALLY SUSHEEL\\Desktop\\new del pxlb\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\\scp_statements.csv\n",
      "Extracting labels...\n",
      "Label distribution:\n",
      "[4898. 2649. 5469. 9514. 5235.]\n",
      "Loading signals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading signals: 1000it [00:31, 32.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 signals with shape (1000, 12, 1000)\n",
      "Extracting DWT features...\n",
      "Extracting DWT features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 125.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWT features shape: (1000, 360)\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 20/20 [00:12<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.5262, Val Loss: 0.5009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 20/20 [00:13<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.4356, Val Loss: 0.4132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 20/20 [00:13<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.4087, Val Loss: 0.3782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 20/20 [00:12<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.3780, Val Loss: 0.3604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 20/20 [00:12<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.3831, Val Loss: 0.3831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 20/20 [00:12<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.3550, Val Loss: 0.4693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.3682, Val Loss: 0.4851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 20/20 [00:11<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.3347, Val Loss: 0.4108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 20/20 [00:14<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.3109, Val Loss: 0.4048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 20/20 [00:10<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.3120, Val Loss: 0.4118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 20/20 [00:11<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.2921, Val Loss: 0.4464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 20/20 [00:12<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.2971, Val Loss: 0.4017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 20/20 [00:12<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 0.2831, Val Loss: 0.4029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 20/20 [00:13<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 0.2589, Val Loss: 0.4310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 20/20 [00:10<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 0.2434, Val Loss: 0.4993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 20/20 [00:12<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 0.2489, Val Loss: 0.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 20/20 [00:11<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 0.2563, Val Loss: 0.5697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 20/20 [00:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 0.2390, Val Loss: 0.4840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 20/20 [00:10<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss: 0.2204, Val Loss: 0.7258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 20/20 [00:10<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss: 0.2195, Val Loss: 0.5306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss: 0.2159, Val Loss: 0.5360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 20/20 [00:13<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss: 0.2061, Val Loss: 0.5623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 20/20 [00:10<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss: 0.2102, Val Loss: 0.6649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 20/20 [00:10<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss: 0.1945, Val Loss: 0.6460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 20/20 [00:10<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss: 0.1970, Val Loss: 0.6458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 20/20 [00:11<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss: 0.1903, Val Loss: 0.7052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 20/20 [00:10<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss: 0.1942, Val Loss: 0.6736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 20/20 [00:10<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss: 0.1940, Val Loss: 0.5816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 20/20 [00:10<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss: 0.1886, Val Loss: 0.7073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 20/20 [00:10<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss: 0.1819, Val Loss: 0.7128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 20/20 [00:10<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss: 0.1803, Val Loss: 0.7034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 20/20 [00:10<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Train Loss: 0.1774, Val Loss: 0.7115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 20/20 [00:10<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Train Loss: 0.1739, Val Loss: 0.7294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 20/20 [00:10<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train Loss: 0.1755, Val Loss: 0.7247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 20/20 [00:10<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Train Loss: 0.1760, Val Loss: 0.7190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 20/20 [00:10<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Train Loss: 0.1689, Val Loss: 0.7160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 20/20 [00:13<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: Train Loss: 0.1731, Val Loss: 0.7194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 20/20 [00:11<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: Train Loss: 0.1717, Val Loss: 0.7255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 20/20 [00:09<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: Train Loss: 0.1628, Val Loss: 0.7289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 20/20 [00:10<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Train Loss: 0.1723, Val Loss: 0.7456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 20/20 [00:10<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: Train Loss: 0.1667, Val Loss: 0.7351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: Train Loss: 0.1667, Val Loss: 0.7377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 20/20 [00:10<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: Train Loss: 0.1642, Val Loss: 0.7517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 20/20 [00:09<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: Train Loss: 0.1674, Val Loss: 0.7605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 20/20 [00:10<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Train Loss: 0.1678, Val Loss: 0.7633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 20/20 [00:10<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: Train Loss: 0.1680, Val Loss: 0.7659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 20/20 [00:10<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss: 0.1687, Val Loss: 0.7567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 20/20 [00:10<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Train Loss: 0.1643, Val Loss: 0.7630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 20/20 [00:10<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Loss: 0.1612, Val Loss: 0.7639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 20/20 [00:10<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 0.1649, Val Loss: 0.7692\n",
      "Evaluating model...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.39      0.28      0.33        39\n",
      "         HYP       0.00      0.00      0.00        16\n",
      "          MI       0.43      0.34      0.38        35\n",
      "        NORM       0.84      0.79      0.82       115\n",
      "        STTC       0.56      0.71      0.62        42\n",
      "\n",
      "   micro avg       0.66      0.58      0.62       247\n",
      "   macro avg       0.44      0.43      0.43       247\n",
      "weighted avg       0.61      0.58      0.59       247\n",
      " samples avg       0.66      0.61      0.63       247\n",
      "\n",
      "\n",
      "Performing SHAP analysis...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPipeline completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# SHAP analysis\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPerforming SHAP analysis...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m sample_data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_test_ecg\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_dwt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m explain_with_shap(model, sample_data, DEVICE)\n\u001b[32m     81\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPipeline completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "# Main execution pipeline\n",
    "def main():\n",
    "    \"\"\"Main pipeline execution\"\"\"\n",
    "    # Configuration\n",
    "    DATA_PATH = r\"C:\\Users\\JANPALLY SUSHEEL\\Desktop\\new del pxlb\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\"\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 50\n",
    "    \n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    # Initialize data processor\n",
    "    processor = PTBXLDataProcessor(DATA_PATH)\n",
    "    \n",
    "    # Load and process data\n",
    "    print(\"Loading database...\")\n",
    "    processor.load_database()\n",
    "    processor.load_scp_statements()\n",
    "    \n",
    "    print(\"Extracting labels...\")\n",
    "    processor.extract_labels()\n",
    "    \n",
    "    print(\"Loading signals...\")\n",
    "    signals, labels = processor.load_signals(limit=1000)  # Limit for demo\n",
    "    \n",
    "    print(\"Extracting DWT features...\")\n",
    "    dwt_features = processor.extract_dwt_features()\n",
    "    \n",
    "    # Prepare data for training\n",
    "    X_train_ecg, X_test_ecg, X_train_dwt, X_test_dwt, y_train, y_test = train_test_split(\n",
    "        signals, dwt_features, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    X_train_ecg, X_val_ecg, X_train_dwt, X_val_dwt, y_train, y_val = train_test_split(\n",
    "        X_train_ecg, X_train_dwt, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    class ECGDWTDataset(Dataset):\n",
    "        def __init__(self, ecg_signals, dwt_features, labels):\n",
    "            self.ecg_signals = ecg_signals\n",
    "            self.dwt_features = dwt_features\n",
    "            self.labels = labels\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.ecg_signals)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return (torch.FloatTensor(self.ecg_signals[idx]),\n",
    "                   torch.FloatTensor(self.dwt_features[idx]),\n",
    "                   torch.FloatTensor(self.labels[idx]))\n",
    "    \n",
    "    train_dataset = ECGDWTDataset(X_train_ecg, X_train_dwt, y_train)\n",
    "    val_dataset = ECGDWTDataset(X_val_ecg, X_val_dwt, y_val)\n",
    "    test_dataset = ECGDWTDataset(X_test_ecg, X_test_dwt, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ECGClassificationModel(dwt_features=dwt_features.shape[1])\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader, NUM_EPOCHS, DEVICE)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "    predictions, true_labels = evaluate_model(model, test_loader, DEVICE)\n",
    "    \n",
    "    # Print results\n",
    "    class_names = ['CD', 'HYP', 'MI', 'NORM', 'STTC']\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predictions, target_names=class_names))\n",
    "    \n",
    "    # SHAP analysis\n",
    "    print(\"\\nPerforming SHAP analysis...\")\n",
    "    sample_data = np.concatenate([X_test_ecg[:10], X_test_dwt[:10]], axis=1)\n",
    "    explain_with_shap(model, sample_data, DEVICE)\n",
    "    \n",
    "    print(\"Pipeline completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
